{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "<br><h2>Session 7b | Ensemble Modeling</h2>\n",
    "<h4>DAT-5303 | Machine Learning</h4>\n",
    "Chase Kusterer - Faculty of Analytics<br>\n",
    "Hult International Business School<br><br><br>\n",
    "\n",
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<h3>Part I: Preparation</h3><br>\n",
    "Run the following code to import necessary packages, load data, and set display options for pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# importing packages\n",
    "########################################\n",
    "import matplotlib.pyplot as plt                      # data visualization\n",
    "import pandas as pd                                  # data science essentials\n",
    "from sklearn.model_selection import train_test_split # train-test split\n",
    "from sklearn.metrics import roc_auc_score            # auc score\n",
    "from sklearn.model_selection import GridSearchCV     # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer              # customizable scorer\n",
    "\n",
    "\n",
    "# new packages\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "\n",
    "\n",
    "########################################\n",
    "# loading data and setting display options\n",
    "########################################\n",
    "# loading data\n",
    "titanic = pd.read_excel('titanic_feature_rich.xlsx')\n",
    "\n",
    "\n",
    "# loading model performance\n",
    "model_performance = pd.read_excel('Classification Model Performance.xlsx')\n",
    "\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "\n",
    "########################################\n",
    "# explanatory variable sets\n",
    "########################################\n",
    "candidate_dict = {\n",
    "\n",
    " # full model\n",
    " 'logit_full'   : ['age', 'sibsp', 'parch', 'fare', 'm_age', 'm_cabin',\n",
    "                   'm_home.dest', 'potential_youth', 'child',\n",
    "                   'number_of_names', 'pclass_1', 'pclass_2', 'female'],\n",
    " \n",
    " # significant variables only\n",
    " 'logit_sig'    : ['age' , 'sibsp', 'm_cabin', 'number_of_names',\n",
    "                   'pclass_1', 'female']\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "########################################\n",
    "# checking previous model performances\n",
    "########################################\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "<strong>User-Defined Functions</strong><br>\n",
    "Run the following code to load the user-defined functions used throughout this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "<strong>Challenge 1</strong><br>\n",
    "Complete the code to split the dataset into training and validation sets using the <em>logit_sig</em> set of explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# train/test split with the logit_sig variables\n",
    "titanic_data   =  titanic.loc[ : , candidate_dict['logit_sig']]\n",
    "titanic_target =  titanic.loc[ : , 'm_boat']\n",
    "\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            titanic_data,\n",
    "            titanic_target,\n",
    "            random_state = 802,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = titanic_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "<h3>Part II: Random Forest</h3><br>\n",
    "A random forest can be thought of as a group of decision trees that are all slightly different. This model type starts by randomly selecting a subset of explanatory variables and building a decision tree. Then, it takes another random subset of explanatory variables and builds another tree. After building several trees, each observation has several different results for its prediction. This can be thought of as giving each tree a vote as to what to predict for each observation.\n",
    "\n",
    "For example, one observation may have been voted positive 80% of the time (the event in question occurred), and voted negative 20% of the time (the event in question did not occur). After all votes have been cast, whichever class (positive or negative) has the most votes wins, and prediction on the observation is complete.<br><br><br>\n",
    "<strong>Challenge 2</strong><br>\n",
    "Build a random forest model using the significant set of explanatory variables ( <em>logit_sig</em> ) and default values for the hyperparameters listed below. Remember, default values are documented in help( ) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a random forest model with default values\n",
    "rf_default = RandomForestClassifier(n_estimators     = 10,\n",
    "                                    criterion        = 'gini',\n",
    "                                    max_depth        = None,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    bootstrap        = True,\n",
    "                                    warm_start       = False,\n",
    "                                    random_state     = 802)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', rf_default_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', rf_default_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rf_default_fit_pred).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "<strong>Challenge 3</strong><br>\n",
    "Write and run the feature importance function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "plot_feature_importances(rf_default_fit,\n",
    "                         train = X_train,\n",
    "                         export = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "Run the following code to write the results of the tuned classification model to model_performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "rf_train_acc = rf_default_fit.score(X_train, y_train).round(4)\n",
    "rf_test_acc  = rf_default_fit.score(X_test, y_test).round(4)\n",
    "rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = rf_default_fit_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model'             : 'Random Forest',\n",
    "                          'Training Accuracy'  : rf_train_acc,\n",
    "                          'Testing Accuracy'   : rf_test_acc,\n",
    "                          'AUC Value'          : rf_auc},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "<strong>Challenge 4</strong><br>\n",
    "Prepare the code below so that it splits the data into training and testing sets on the full set of explanatory variables (<em>logit_full</em>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# train/test split with the logit_sig variables\n",
    "titanic_data   =  titanic.loc[ : , candidate_dict['logit_full']]\n",
    "titanic_target =  titanic.loc[ : , 'm_boat']\n",
    "\n",
    "\n",
    "# train/test split\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "            titanic_data,\n",
    "            titanic_target,\n",
    "            random_state = 802,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = titanic_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "Run the following code to develop a random forest using default values for each hyperparameter listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a random forest model with default values\n",
    "rf_default_full = RandomForestClassifier(n_estimators     = 10,\n",
    "                                         criterion        = 'gini',\n",
    "                                         max_depth        = None,\n",
    "                                         min_samples_leaf = 1,\n",
    "                                         bootstrap        = True,\n",
    "                                         warm_start       = False,\n",
    "                                         random_state     = 802)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "rf_default_full_fit = rf_default_full.fit(X_train_full, y_train_full)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_full_pred = rf_default_full_fit.predict(X_test_full)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', rf_default_full_fit.score(X_train_full, y_train_full).round(4))\n",
    "print('Testing  ACCURACY:', rf_default_full_fit.score(X_test_full, y_test_full).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test_full,\n",
    "                                          y_score = rf_default_full_pred).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "<strong>Challenge 5</strong><br>\n",
    "Write and run the feature importance function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# plotting feature importance\n",
    "plot_feature_importances(rf_default_full_fit,\n",
    "                         train = X_train_full,\n",
    "                         export = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "<strong>Random Forest with Tuned Hyperparameters</strong><br>\n",
    "Run the following code to automate hyperparameter optimization for a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "estimator_space  = pd.np.arange(100, 1100, 250)\n",
    "leaf_space       = pd.np.arange(1, 31, 10)\n",
    "criterion_space  = ['gini', 'entropy']\n",
    "bootstrap_space  = [True, False]\n",
    "warm_start_space = [True, False]\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'n_estimators'     : estimator_space,\n",
    "              'min_samples_leaf' : leaf_space,\n",
    "              'criterion'        : criterion_space,\n",
    "              'bootstrap'        : bootstrap_space,\n",
    "              'warm_start'       : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_forest_grid = RandomForestClassifier(random_state = 802)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "full_forest_cv = GridSearchCV(estimator  = full_forest_grid,\n",
    "                              param_grid = param_grid,\n",
    "                              cv         = 3,\n",
    "                              scoring    = make_scorer(roc_auc_score,\n",
    "                                           needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "full_forest_cv.fit(titanic_data, titanic_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", full_forest_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", full_forest_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "Automated hyperparameter optimization can take a long time. In order to avoid having to run this each time a script is loaded, it is a good practice to:<br>\n",
    "\n",
    "1. Run an automated hyperparameter optimization technique and record its results\n",
    "2. Comment out (but not delete) the hyperparameter optimization code\n",
    "3. Manually set each hyperparameter when building a tuned model\n",
    "\n",
    "<br>\n",
    "This will help alleviate processing bottlenecks while allowing you to uncomment and rerun the optimization algorithm as needed.<br><br><br>\n",
    "<strong>Challenge 6</strong><br>\n",
    "Instead of utilizing the <em>.best_estimator_</em> attribute of <em>GridSearchCV</em>, manually input the optimal set of hyperparameters when instantiating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_rf_tuned = RandomForestClassifier(bootstrap        = True,\n",
    "                                       criterion        = 'gini',\n",
    "                                       min_samples_leaf = 11,\n",
    "                                       n_estimators     = 850,\n",
    "                                       warm_start       = True,\n",
    "                                       random_state     = 802)\n",
    "\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "full_rf_tuned_fit = full_rf_tuned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "full_rf_tuned_pred = full_rf_tuned_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', full_rf_tuned_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', full_rf_tuned_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = full_rf_tuned_pred).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "Run the following code to write the results of the tuned classification model to model_performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "rf_train_acc = full_rf_tuned_fit.score(X_train, y_train).round(4)\n",
    "rf_test_acc  = full_rf_tuned_fit.score(X_test, y_test).round(4)\n",
    "rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = full_rf_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model'             : 'Tuned Random Forest',\n",
    "                          'Training Accuracy'  : rf_train_acc,\n",
    "                          'Testing Accuracy'   : rf_test_acc,\n",
    "                          'AUC Value'          : rf_auc},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "<h3>Part III: Gradient Boosted Machines</h3><br>\n",
    "Gradient boosted machines (GBMs) are like decision trees, but instead of starting fresh with each iteration, they learn from mistakes made in previous iterations. Unlike random forest, GBMs use a row-wise penalty instead of a column-wise penalty, reweighting each row instead of each column.<br><br>\n",
    "<strong>Challenge 7</strong><br>\n",
    "Develop a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\">GradientBoostingClassifier</a> model with default values for the hyperparameters listed below. Remember, default values are documented in help( ) files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_default = GradientBoostingClassifier(loss          = 'deviance',\n",
    "                                              learning_rate = 0.1,\n",
    "                                              n_estimators  = 100,\n",
    "                                              criterion     = 'friedman_mse',\n",
    "                                              max_depth     = 3,\n",
    "                                              warm_start    = False,\n",
    "                                              random_state  = 802)\n",
    "\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "full_gbm_default_fit = full_gbm_default.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "full_gbm_default_pred = full_gbm_default_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', full_gbm_default_fit.score(X_train, y_train).round(4))\n",
    "print('Testing ACCURACY :', full_gbm_default_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = full_gbm_default_pred).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "Notice above that we are using <em>friedman_mse</em> as the criterion. Friedman proposed that instead of focusing on one MSE value for the entire tree, the algorithm should localize its optimal MSE for each region of the tree. See <a href=\"https://statweb.stanford.edu/~jhf/ftp/stobst.pdf\">this research paper</a> for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "gbm_train_acc = full_gbm_default_fit.score(X_train, y_train).round(4)\n",
    "gbm_test_acc  = full_gbm_default_fit.score(X_test, y_test).round(4)\n",
    "gbm_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = full_gbm_default_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model'             : 'GBM',\n",
    "                          'Training Accuracy'  : gbm_train_acc,\n",
    "                          'Testing Accuracy'   : gbm_test_acc,\n",
    "                          'AUC Value'          : gbm_auc},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "<strong>Challenge 8</strong><br>\n",
    "Complete the code to perform hyperparameter optimization on a GBM model using the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "learn_space     = pd.np.arange(0.1, 1.6, 0.3)\n",
    "estimator_space = pd.np.arange(50, 250, 50)\n",
    "depth_space     = pd.np.arange(1, 10)\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'learning_rate' : learn_space,\n",
    "              'max_depth'     : depth_space,\n",
    "              'n_estimators'  : estimator_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_grid = GradientBoostingClassifier(random_state = 802)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "full_gbm_cv = GridSearchCV(estimator  = full_gbm_grid,\n",
    "                           param_grid = param_grid,\n",
    "                           cv         = 3,\n",
    "                           scoring    = make_scorer(roc_auc_score,\n",
    "                                        needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "full_gbm_cv.fit(titanic_data, titanic_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(\"Tuned Parameters  :\", full_gbm_cv.best_params_)\n",
    "print(\"Tuned Training AUC:\", full_gbm_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "<strong>Challenge 9</strong><br>\n",
    "Manually input the optimal set of hyperparameters when instantiating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "gbm_tuned = GradientBoostingClassifier(learning_rate = 0.1,\n",
    "                                       max_depth     = 2,\n",
    "                                       n_estimators  = 100,\n",
    "                                       random_state  = 802)\n",
    "\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "gbm_tuned_fit = gbm_tuned.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "gbm_tuned_pred = gbm_tuned_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', gbm_tuned_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', gbm_tuned_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = gbm_tuned_pred).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "<strong>Challenge 10</strong><br>\n",
    "Write the results of the tuned classification model to model_performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "gbm_train_acc = gbm_tuned_fit.score(X_train, y_train).round(4)\n",
    "gbm_test_acc  = gbm_tuned_fit.score(X_test, y_test).round(4)\n",
    "gbm_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = gbm_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model'             : 'Tuned GBM',\n",
    "                          'Training Accuracy'  : gbm_train_acc,\n",
    "                          'Testing Accuracy'   : gbm_test_acc,\n",
    "                          'AUC Value'          : gbm_auc},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<br>\n",
    "Run the following code to view each model's performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "model_performance.sort_values(by = 'AUC Value',\n",
    "                              ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "Run the following code to save model_performance as an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# saving the DataFrame to Excel\n",
    "model_performance_df.to_excel('Classification Model Performance.xlsx',\n",
    "                              index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}